\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{booktabs}
\geometry{margin=1in}
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}

\begin{document}

\begin{center}
    {\LARGE Sport Video Analysis} \\
    \vspace{1em}
    Rajmonda Bardhi (Student ID: 2071810) \\
    Computer Vision \\
    University of Padova \\
    \vspace{1em}
    February 2025
\end{center}

\section{Introduction}

Computer vision has become an increasingly valuable tool for sports analytics in recent years. Football (soccer), being the world's most popular sport, has attracted considerable research interest owing to its tactical complexity and the practical insights that automated analysis can deliver to coaches, analysts, and broadcasters. Compared to more spatially constrained sports like tennis or basketball, football poses distinct challenges: the pitch covers a large area, camera perspectives shift throughout a match, players move unpredictably, occlusions are frequent, and the sheer number of on-field participants makes robust real-time detection difficult. Overcoming these obstacles, however, carries significant practical value, as automated systems can support scouting, tactical planning, match broadcasting, and supporter engagement.

Match analysis has historically depended on manual annotation, where analysts watch recordings and log player positions, passes, and movement trends. This labor-intensive process is slow and susceptible to human error. Computer vision methods offer a path to reducing this workload while extracting richer positional data at frame-level temporal resolution. Contemporary solutions often employ deep learning---convolutional neural networks (CNNs) or vision transformers---for object detection and classification. Although these approaches deliver state-of-the-art results, they impose substantial requirements: heavy computational resources, large labeled datasets, and architectures that function largely as black boxes. In many practical settings, a lighter, more transparent pipeline can produce useful results without the overhead of training or GPU infrastructure.

This project presents a football video analysis system built around three objectives:

\begin{enumerate}
    \item \textbf{Player Detection:} Identify all players on the pitch in every frame of a broadcast video, handling conditions such as motion blur, partial occlusion, and varying camera zoom.
    \item \textbf{Team Classification:} Assign each detected player to one of two teams on the basis of jersey color, enabling team-level tactical separation.
    \item \textbf{Heatmap Generation:} Accumulate spatial detections over time to produce per-team heatmaps that reveal areas of concentrated activity, exposing defensive and offensive tendencies.
\end{enumerate}

The entire system is implemented in C++ with the OpenCV library, deliberately relying on classical computer vision rather than deep learning. The pipeline combines background subtraction, HSV-based field masking, morphological refinement, contour analysis, and unsupervised clustering to produce an efficient, interpretable system that requires no labeled training data. While classical methods may not match the raw accuracy of modern neural detectors, their speed, tunability, and transparency make them well suited to constrained or educational environments.

Several design choices in this project draw directly on techniques covered in the Computer Vision course lectures. Specifically, the pipeline employs HSV color segmentation (Lab~3, Lecture~10\_3), morphological operators for mask refinement (Lecture~10\_1), region-based segmentation via contour analysis (Lecture~10\_2), k-means clustering for unsupervised team assignment (Lecture~11\_1), and Gaussian Mixture Model background subtraction (Lecture~11\_2).

For evaluation, we generated pseudo-ground truth by running YOLOv8---a leading deep object detector pretrained on the COCO dataset---on the same video sequences. Detections of the ``person'' class served as reference bounding boxes, against which our pipeline was compared using Precision, Recall, F1-score, and mean Intersection-over-Union (mIoU). This allowed both counting-based and spatial evaluation of detection quality.

Taken as a whole, the project constitutes a full end-to-end workflow: from raw video input, through modular classical CV algorithms, to interpretable outputs including CSV detection logs and tactical heatmaps. The system was designed not only to function correctly, but also to be extensible---for instance through the addition of tracking algorithms or lightweight deep modules. The remainder of this report covers methodology, implementation, evaluation, and discussion.

\newpage

\section{Usage Instructions}

The project ships the main detection pipeline executable (\texttt{detect}), along with standalone utilities for YOLO label conversion (\texttt{yolo\_to\_csv}) and IoU-based evaluation (\texttt{detection\_evaluator}). The source is organized into modular files: \texttt{main.cpp}, \texttt{player\_detection.cpp}, \texttt{team\_classification.cpp}, and \texttt{player\_heatmap.cpp}.

\subsection{Building with CMake (recommended)}

From the project root:

\begin{verbatim}
mkdir build
cd build
cmake ..
make
\end{verbatim}

This produces the \texttt{detect} executable.

\subsection{Alternative: direct compilation}

\begin{verbatim}
g++ -std=c++17 main.cpp player_detection.cpp \
    team_classification.cpp player_heatmap.cpp \
    `pkg-config --cflags --libs opencv4` -o detect

g++ -std=c++17 detection_evaluator.cpp -o detection_evaluator

g++ -std=c++17 yolo_to_csv.cpp \
    `pkg-config --cflags --libs opencv4` -o yolo_to_csv
\end{verbatim}

\subsection{Running the Detection Pipeline}

\begin{verbatim}
./detect input_video.mp4
\end{verbatim}

Outputs \texttt{ours.csv} with columns:

\begin{verbatim}
frame, x1, y1, x2, y2, team
\end{verbatim}

and saves heatmap images to disk.

\subsection{Running the Evaluation Tool}

Compare against YOLO pseudo-ground truth (\texttt{yolo.csv}):

\begin{verbatim}
./detection_evaluator ours.csv yolo.csv [iou_thr=0.5] \
    [ours_offset=0] [yolo_offset=0]
\end{verbatim}

Example (apply a $-1$ frame offset to YOLO):

\begin{verbatim}
./detection_evaluator ours.csv yolo.csv 0.5 0 -1
\end{verbatim}

The tool prints Precision, Recall, F1-score, and mean IoU (mIoU).

\newpage

\section{Methodology}

This section describes the complete processing pipeline, from raw broadcast video to bounding-box detections, team labels, and heatmap visualizations. Each stage is discussed in terms of its purpose, implementation, and the course material that motivated the design.

\subsection{Code Organization}

The codebase is split into focused modules for testability and clarity. \texttt{main.cpp} handles video I/O and the frame loop. \textbf{(i)}~\texttt{player\_detection.cpp / player\_detection.h} implements field masking, background subtraction, morphological refinement, contour extraction, and box filtering. \textbf{(ii)}~\texttt{team\_classification.cpp / team\_classification.h} handles team assignment through CIELab color features, k-means clustering, temporal anchoring, and confidence-based smoothing. \textbf{(iii)}~\texttt{player\_heatmap.cpp / player\_heatmap.h} accumulates per-team spatial density and exports visualization images. This separation keeps each module independently tunable while \texttt{main.cpp} remains lightweight.

\subsection{Pipeline Overview}

The pipeline processes each frame through the following stages:

\begin{enumerate}
    \item Frame acquisition from the video stream.
    \item HSV conversion and green field segmentation via color thresholding.
    \item Morphological dilation and erosion to refine the field mask.
    \item Area-based contour filtering to isolate the playing field.
    \item Background subtraction (MOG2) to isolate moving objects.
    \item Player color mask generation with shadow and black-pixel suppression.
    \item Dilation to connect fragmented player pixels.
    \item Combination of motion and color masks, restricted to the field region.
    \item Morphological opening to remove small noise blobs.
    \item Contour extraction and bounding box computation.
    \item Box filtering by area, size, and aspect ratio.
    \item Agglomerative merging of overlapping boxes and containment suppression.
    \item Team classification via k-means on CIELab jersey color descriptors.
    \item Confidence-based temporal smoothing with nearest-neighbor tracking.
    \item Heatmap accumulation and density visualization.
    \item CSV logging and annotated frame display.
\end{enumerate}

\subsection{Frame Acquisition}

Frames are read sequentially through the \texttt{cv::VideoCapture} interface. The system accepts standard broadcast formats (MP4, AVI) and processes every frame without subsampling, preserving temporal continuity for both detection and heatmap accumulation. Playback speed is synchronized to the video's native frame rate.

\subsection{Field Segmentation}

Isolating the pitch from non-field elements (stands, advertising boards, score overlays) is a prerequisite for reliable detection. We operate in HSV color space, which decouples chromatic information (hue) from illumination (value), making green detection robust across varying lighting conditions (Lab~3 ``Color segmentation''; Lecture~10\_3 ``Thresholding'').

A binary field mask is produced by thresholding in HSV with the green range $H \in [40, 90]$, $S \geq 40$, $V \geq 40$. This range captures the dominant grass color while excluding non-field green objects that fall outside these bounds.

Morphological dilation with a $5 \times 5$ rectangular kernel fills small holes in the initial mask, followed by four successive erosion passes with the same kernel to shrink noisy edges and remove thin protrusions (Lecture~10\_1 ``Morphological operators'').

External contours are then extracted from the eroded mask. Only contours with area exceeding 1000~pixels are retained and filled into the final field mask (Lecture~10\_2 ``Intro to segmentation''). This area-based filtering discards small spurious green regions while preserving the main playing field.

\subsection{Background Subtraction}

Static field pixels carry no information about player positions. To isolate dynamic regions we apply OpenCV's MOG2 background subtractor (Lecture~11\_2 ``Density estimation'': Gaussian Mixture Models for pixel-level background modeling). MOG2 maintains a per-pixel statistical model and flags deviations from the learned background, emphasizing moving objects such as players and referees while suppressing the static pitch. A low learning rate of $0.01$ is used to keep the background model stable during slow camera pans.

\subsection{Player Mask with Shadow Suppression}

Within the field-masked region, a second color-based segmentation step isolates non-field pixels that likely belong to players. Three categories of pixels are suppressed (Lab~3 ``Color segmentation''; Lecture~10\_3 ``Thresholding''):

\begin{itemize}
    \item \textbf{Green field pixels:} the same HSV range ($H \in [40, 90]$, $S \geq 40$, $V \geq 40$) used for field segmentation.
    \item \textbf{Shadow pixels:} all pixels with $V < 50$ are masked out, as shadows cast on the field have low brightness regardless of hue.
    \item \textbf{Near-black pixels:} pixels with $H < 10$, $S < 10$, $V < 10$ are suppressed to remove dark noise artifacts.
\end{itemize}

These three masks are combined with bitwise OR and then inverted, yielding a mask of pixels that are neither green, shadowed, nor black---i.e., likely player-colored regions. Dilation with an $11 \times 11$ rectangular structuring element ($\text{radius} = 5$) connects nearby player-colored pixels into cohesive blobs (Lecture~10\_1).

\subsection{Mask Combination and Field Restriction}

The foreground motion mask (from MOG2) and the player color mask are combined with a bitwise AND. A second bitwise AND with the field mask ensures that no detection can originate outside the pitch (Lecture~10\_2: combining multiple segmentation cues). This dual-mask approach leverages both temporal (motion) and spectral (color) information, reducing false positives from spectators, camera operators, and stadium structures.

\subsection{Morphological Refinement}

A morphological opening operation is applied to the combined mask using a $5 \times 5$ elliptical kernel (Lecture~10\_1 ``Morphological operators''). Opening---erosion followed by dilation---removes thin noise protrusions and small artifact blobs while preserving the shape of larger connected regions corresponding to players.

\subsection{Contour Extraction and Bounding Boxes}

From the refined mask, external contours are extracted with \texttt{cv::findContours} and each is approximated by its bounding rectangle. This yields a set of candidate player bounding boxes, which still includes noise artifacts at this stage.

\subsection{Box Filtering and Merging}

The raw candidate boxes are refined through several filters:

\begin{itemize}
    \item \textbf{Minimum area:} Contours with area below 30~pixels are discarded as noise.
    \item \textbf{Size constraints:} Boxes outside $w \in [10, 100]$ and $h \in [20, 200]$ pixels are rejected, corresponding to plausible player dimensions at typical broadcast resolutions.
    \item \textbf{Aspect ratio:} Boxes where $h < w$ are filtered out. Football players are taller than wide; this constraint rejects flat shadow artifacts and horizontal noise regions (Lecture~10\_2: shape descriptors for region properties).
    \item \textbf{Overlap merging:} When multiple contours cover a single player, their bounding boxes are iteratively merged using an agglomerative approach---unifying boxes that overlap or contain each other (Lecture~11\_1: agglomerative clustering). The algorithm repeatedly scans for overlapping pairs until no further merges occur.
    \item \textbf{Containment suppression:} After merging, any box fully contained inside a larger box is removed, keeping only the outermost candidate.
\end{itemize}

\subsection{Team Classification}

With player bounding boxes established, the next task is assigning each player to a team. Since YOLO ground truth provides no team labels, we implemented unsupervised classification:

\begin{enumerate}
    \item Each bounding box region is extracted and resized to $32 \times 64$ pixels.
    \item Only the \textbf{upper 60\%} of the ROI is used, corresponding to the jersey/shirt area. The lower body (shorts, legs, feet) is excluded as it adds noise to the color descriptor.
    \item The jersey region is converted to both HSV (for masking) and CIELab color space. CIELab separates luminance ($L$) from chromatic channels ($a$, $b$) and provides perceptually uniform color distances (Lecture~11\_1 ``K-means'': feature space selection).
    \item Pixels resembling the pitch green and shadow pixels ($V < 50$) within the ROI are excluded via HSV filtering.
    \item The \textbf{median} Lab values of the remaining non-excluded pixels form a compact, outlier-robust descriptor for each detection.
    \item K-means clustering with $k = 2$ partitions all detections in a frame into two groups corresponding to the two teams (Lecture~11\_1: k-means partitioning). The \texttt{KMEANS\_PP\_CENTERS} initialization and 5 random restarts reduce sensitivity to local minima.
\end{enumerate}

To prevent frame-to-frame label flipping, \textbf{temporal anchors} are maintained: cluster centers are updated with an exponential moving average ($\alpha = 0.1$) over the first 10~frames. After this initialization period, new k-means cluster indices are mapped to stable team IDs by matching each cluster center to its nearest anchor using Euclidean distance.

Additionally, a \textbf{confidence-based temporal smoothing} mechanism is applied. For each detection, the ratio of distances to the assigned cluster center versus the opposing cluster center is computed. When this confidence ratio exceeds 0.7 (indicating the player lies near the decision boundary), the system falls back to the team label from the closest tracked player in the previous frame, identified via nearest-neighbor matching on bounding box centers. This prevents noisy reassignment of borderline players.

\subsection{Heatmap Generation}

Per-team heatmaps are generated by accumulating detections over the entire video (Lecture~11\_2 ``Density estimation''):

\begin{itemize}
    \item For each classified detection, a filled circle of radius 20~pixels is drawn at the bounding box center on a floating-point RGB accumulator image. The circle color is indexed by team label: red for Team~A, blue for Team~B, green for Unknown.
    \item After all frames are processed, the accumulator is Gaussian-blurred with $\sigma = 15$ (Lecture~06\_1 ``Spatial filtering'') and normalized to $[0, 255]$ to produce a smooth density map.
    \item The result is saved both as a standalone heatmap image (\texttt{combined\_heatmap.png}) and as a 50/50 blended overlay on the first video frame (\texttt{heatmap\_overlay.png}).
\end{itemize}

These visualizations reveal tactical patterns---pressing zones, defensive concentration, attacking corridors---in an intuitive, human-readable format.

\subsection{Output and Logging}

The system produces the following artifacts each run:

\begin{itemize}
    \item \texttt{ours.csv}: per-frame bounding boxes with team labels (columns: \texttt{frame, x1, y1, x2, y2, team}).
    \item Real-time annotated video display with color-coded boxes (red = Team~A, blue = Team~B, green = Unknown) and team labels above each box.
    \item Heatmap images showing per-team spatial occupancy.
\end{itemize}

\subsection{Evaluation Setup}

To produce the YOLO pseudo-ground truth baseline we used the Ultralytics YOLOv8 implementation:

\begin{enumerate}
    \item Install: \texttt{pip install ultralytics}
    \item Run inference:
    \begin{verbatim}
yolo detect predict model=yolov8n.pt \
     source=input_video.mp4 save_txt=True
    \end{verbatim}
    \item Convert YOLO TXT labels to CSV with the provided converter:
    \begin{verbatim}
./yolo_to_csv runs/detect/predict/labels \
     input_video.mp4 yolo.csv
    \end{verbatim}
    \item Evaluate:
    \begin{verbatim}
./detection_evaluator ours.csv yolo.csv 0.5 0 -1
    \end{verbatim}
\end{enumerate}

The \texttt{yolo\_to\_csv} utility reads YOLO-format label files (class, center-x, center-y, width, height in normalized coordinates) and converts them to absolute pixel coordinates in CSV format, filtering only class~0 (person). A dedicated C++ evaluation program (\texttt{detection\_evaluator.cpp}) parses both CSVs, performs greedy IoU-based matching per frame, and computes Precision, Recall, F1-score, and mIoU. This keeps the evaluation fully reproducible and self-contained within the project.

\newpage

\section{Dataset}

Experiments were conducted on the DFL Bundesliga broadcast dataset, which contains 460 short MP4 clips (approximately 30~seconds each) with accompanying CSV metadata. The footage comprises standard TV broadcasts featuring dynamic camera motion, multiple visible players, and a variety of stadium and lighting conditions, making it well suited for evaluating player detection and team-level analysis.

We selected a representative subset of sequences (football1--football6) covering different match conditions: varying illumination, camera zoom levels, player density, and team kit colors. These sequences were used for both qualitative assessment (visual inspection of detections and heatmaps) and quantitative evaluation against YOLOv8 pseudo-ground truth. YOLO was run on the identical video files at their original resolution to ensure frame-aligned comparison.

\newpage

\section{Results}

The system was evaluated on representative sequences from the Bundesliga broadcast dataset. Evaluation covered both quantitative comparison with YOLOv8 pseudo-ground truth and qualitative inspection of annotated frames and heatmaps.

\subsection{Quantitative Evaluation}

Table~\ref{tab:results} summarizes the detection metrics computed by the C++ evaluation tool (\texttt{detection\_evaluator}) at an IoU threshold of 0.5.

\begin{table}[h]
\centering
\caption{Detection pipeline vs.\ YOLOv8 baseline (IoU threshold = 0.5).}
\label{tab:results}
\begin{tabular}{lc}
\toprule
Metric & Value \\
\midrule
True Positives (TP) & --- \\
False Positives (FP) & --- \\
False Negatives (FN) & --- \\
Precision & --- \\
Recall & --- \\
F1-score & --- \\
mIoU & --- \\
\bottomrule
\end{tabular}
\end{table}

% TODO: Fill in the table above with actual evaluation output from:
% ./detection_evaluator ours.csv yolo.csv 0.5 0 -1

\subsection{Qualitative Evaluation}

Visual inspection of annotated frames confirmed that players were generally localized with tight bounding boxes matching their silhouettes. Team colors were correctly assigned in the majority of cases. Errors tended to cluster in crowded regions or under harsh lighting, but did not materially degrade overall output quality. Heatmaps provided clear, intuitive depictions of team positioning that were consistent with the observed flow of play.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{detection_example.png}
\caption{Detected players with team-colored bounding boxes on a broadcast frame.}
\label{fig:detection}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{heatmap_example.png}
\caption{Per-team heatmap showing areas of concentrated player activity.}
\label{fig:heatmap}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{heatmap_overlay_example.png}
\caption{Heatmap overlaid on the pitch for tactical visualization.}
\label{fig:overlay}
\end{figure}

\subsection{Per-Frame Stability}

Across individual frames the pipeline exhibited stable behavior. The temporal anchoring mechanism for team classification ensured consistent team label assignments throughout each video. The confidence-based smoothing prevented noisy reassignment of borderline players between frames. No catastrophic failures (e.g.\ frames with zero detections) were observed, and variance across frames remained limited.

\subsection{Key Design Decisions and Their Impact}

Several design choices had a notable effect on detection quality:

\begin{itemize}
    \item \textbf{Area-based field contour filtering} (threshold of 1000~pixels) effectively discarded small spurious green regions outside the pitch while preserving the main playing field.
    \item \textbf{Shadow suppression} via low-Value HSV filtering ($V < 50$) removed ghost detections near players in direct sunlight without masking dark-jersey players, which retain distinguishable color characteristics.
    \item \textbf{Three-mask exclusion} (green + shadow + black pixels) in the player segmentation stage provided cleaner player-colored regions than using green exclusion alone.
    \item \textbf{Multiple erosion passes} (four iterations) on the field mask aggressively removed noisy edges, producing a cleaner field boundary at the cost of slight field area reduction.
    \item \textbf{Morphological opening} on the combined mask eliminated thin noise protrusions and small blobs that would otherwise register as false player detections.
    \item \textbf{Agglomerative box merging} with containment suppression prevented fragmented detections from producing multiple boxes for a single player.
\end{itemize}

\subsection{Team Classification Performance}

Because YOLOv8 does not supply team labels, classification was evaluated qualitatively. Players were consistently grouped into the correct two clusters based on jersey color. The combination of temporal anchoring (exponential moving average over initial frames) and confidence-based smoothing with nearest-neighbor tracking prevented the label-flipping problem common in per-frame clustering. Using the median Lab value rather than the mean improved robustness to outlier pixels from partial occlusion or noisy ROI boundaries. Misclassification occasionally occurred when jersey colors closely resembled the pitch (e.g.\ green kits) or when lighting caused strong color distortion. Overall, the team separation was reliable enough to support per-team heatmaps and tactical interpretation.

\subsection{Heatmap Results}

The heatmaps produced meaningful tactical visualizations. Gaussian smoothing with $\sigma = 15$ converted discrete detection points into continuous density surfaces that clearly revealed areas of concentrated activity. The team-colored encoding (red vs.\ blue channels) allowed simultaneous visualization of both teams' spatial distributions. These patterns demonstrate the system's practical utility: even when individual detections are imperfect, aggregation over time reveals robust strategic tendencies.

\subsection{Computational Performance}

The pipeline was designed for efficiency. On a standard laptop CPU (no GPU), it processed video at approximately real-time playback speed. The per-frame cost is dominated by background subtraction and k-means clustering, both of which are computationally lightweight operations. This throughput suggests that near-real-time match analysis is feasible on commodity hardware.

\subsection{Summary of Findings}

\begin{itemize}
    \item Classical CV pipeline capable of detecting players and assigning team labels without training data or GPU hardware.
    \item Multi-cue detection combining MOG2 background subtraction with HSV color segmentation, restricted to the segmented field region.
    \item Robust team classification using k-means on CIELab jersey features with temporal anchoring and confidence-based smoothing.
    \item Informative per-team heatmaps capturing tactical formations and spatial occupancy patterns.
    \item Efficient CPU-only execution suitable for commodity hardware.
\end{itemize}

\newpage

\section{Discussion}

The results demonstrate that the proposed pipeline produces meaningful detections and tactical visualizations using exclusively classical computer vision techniques. This section reflects on strengths, weaknesses, and the broader context of the approach.

\subsection{Strengths}

A primary advantage is the system's lightweight footprint. Written entirely in C++ with OpenCV, it runs on a standard CPU without specialized hardware, making it deployable in settings where GPU resources are unavailable or where interpretability takes priority over raw accuracy.

Equally important is pipeline transparency. Every stage---field segmentation, background subtraction, contour analysis, clustering---is individually inspectable. Unlike deep networks that operate as opaque functions, our system allows a practitioner to understand \emph{why} a detection was produced or \emph{why} a player was assigned to a given team. This property is particularly valuable in academic and educational contexts.

The multi-cue approach (combining motion and color evidence, restricted to the field region) provides redundancy: a player must appear in both the MOG2 foreground mask and the non-green color mask to be detected, which naturally suppresses many categories of false positives.

The team classification module addresses a practical need that YOLO-based detectors do not solve: separating players by team. The unsupervised k-means approach with temporal stabilization provides this capability without any labeled training data.

\subsection{Weaknesses and Limitations}

The reliance on color segmentation makes the system sensitive to lighting variation and kit colors. Green jerseys that resemble the pitch confuse the classification module, and strong shadows can introduce noisy foreground masks despite the shadow suppression step.

The system performs per-frame detection without persistent tracking or re-identification. Individual players cannot be followed across time, precluding trajectory-based statistics such as distance covered or sprint speed. The nearest-neighbor matching used for confidence smoothing provides frame-to-frame association but does not constitute full tracking.

The strict aspect ratio constraint ($h > w$) may reject valid detections of players in non-upright poses (diving goalkeepers, sliding tackles, crouching players).

Evaluation depends on YOLOv8 pseudo-ground truth rather than human-annotated labels. YOLO itself is imperfect, so reported metrics should be read as relative performance rather than absolute accuracy.

Scaling to full-length matches or multi-camera setups may introduce additional stability challenges beyond what was tested on short clips.

\subsection{Comparison with Deep Learning}

State-of-the-art detectors such as YOLOv8, Faster R-CNN, and CenterNet achieve higher accuracy and robustness, and can integrate tracking and re-identification modules. However, they demand GPU hardware, extensive training data, and considerable computational resources. Our results show that classical methods, when carefully engineered, can still deliver useful performance---occupying a niche where efficiency, interpretability, and accessibility matter more than peak accuracy.

\subsection{Implications for Football Analytics}

The practical value of the system lies primarily in its heatmap output. Even imperfect per-frame detections, when aggregated over time, produce smooth density maps that reveal consistent tactical patterns---pressing zones, defensive compactness, attacking corridors. For coaches and analysts, this lightweight visualization capability can serve as a quick tool for studying formations and match dynamics without expensive infrastructure.

\subsection{Future Directions}

Several extensions could strengthen the system:

\begin{itemize}
    \item \textbf{Jersey--pitch robustness:} Advanced color models or texture features could reduce confusion when kits resemble the field.
    \item \textbf{Temporal tracking:} Kalman filters, SORT, or optical-flow methods would enable player identity persistence and richer per-player statistics.
    \item \textbf{Convex hull field masking:} Wrapping the largest field contour in a convex hull could better handle irregular pitch boundaries and eliminate edge artifacts.
    \item \textbf{Morphological closing:} A vertically elongated closing kernel could bridge shirt--trouser gaps on players wearing multi-colored kits.
    \item \textbf{Hybrid deep integration:} Lightweight pretrained networks could augment detection while preserving overall efficiency.
    \item \textbf{Adaptive thresholding:} Automatically tuning HSV ranges and kernel sizes to the video's lighting and grass color would improve generalization.
\end{itemize}

\newpage

\section{Conclusion}

This project demonstrates that a complete, end-to-end football video analysis system can be built from classical computer vision primitives. Starting from raw broadcast footage, the pipeline detects players, assigns them to teams, and generates per-team tactical heatmaps---all without training data or GPU hardware.

The pipeline combines multiple complementary techniques: MOG2 background subtraction for motion detection (Lecture~11\_2), HSV color segmentation for field isolation and player masking (Lab~3, Lecture~10\_3), morphological operators for mask refinement (Lecture~10\_1), contour-based region analysis (Lecture~10\_2), k-means clustering for unsupervised team classification (Lecture~11\_1), and Gaussian smoothing for heatmap visualization (Lecture~06\_1). Each technique addresses a specific challenge in the detection pipeline, and their combination produces a system that is greater than the sum of its parts.

While deep learning continues to dominate the field, this work highlights the enduring relevance of interpretable, efficient algorithms, particularly in contexts where computational resources or labeled data are limited. The results show that even a lightweight pipeline can capture meaningful patterns of team behavior, producing both quantitative metrics and intuitive visualizations. With future extensions---tracking, adaptive thresholding, hybrid deep integration---the system could evolve into a practical analytical tool, bridging the gap between raw match footage and actionable football intelligence.

\newpage

\section{Contributions}

This work was completed by a single contributor. All components of the system---methodology design, C++ implementation (player detection, team classification, heatmap generation, evaluation tool), parameter tuning, experimental evaluation, and report preparation---were carried out by \textbf{Rajmonda Bardhi (2071810)}.

\subsection*{Compliance Notes}

\begin{itemize}
    \item The project is implemented in \textbf{C++ with OpenCV}; Python was used only for \textbf{Deep Learning} auxiliary steps (generating YOLO pseudo-ground truth), consistent with course guidelines.
    \item \textbf{CMake} configuration is provided and the code compiles on the \textbf{Virtual Lab} environment.
    \item Only standard libraries and OpenCV were used; \textbf{no external source code} written by third parties is included.
\end{itemize}

\end{document}
